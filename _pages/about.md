---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Senior Research Scientist at Google DeepMind. My recent research interests are: 1) Enabling Gemini to power breakthrough applications through tool-use and RL, 2) Improving methods for privacy-preserving machine learning.

News
======

* [12/17/2025] We released the [Gemini 3 Flash Preview](https://blog.google/products-and-platforms/products/gemini/gemini-3-flash/)! I'm happy to have contributed to some of the agentic training for the model.

* [10/22/2025] We released the Gemini 2.5 Computer Use Model on Google [AIS](https://ai.google.dev/gemini-api/docs/computer-use) and [Vertex](https://cloud.google.com/vertex-ai/generative-ai/docs/computer-use) [[blog](https://blog.google/technology/google-deepmind/gemini-computer-use-model), [additional info](https://storage.googleapis.com/deepmind-media/gemini/computer_use_eval_additional_info.pdf)].

* [09/18/2025] Our work [Scaling Embedding Layers in Language Models](https://arxiv.org/abs/2502.01637) and [Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries](https://arxiv.org/abs/2506.07555) have been accepted to NeurIPS 2025!

* [08/21/2025] I will be serving as an Area Chair for ICLR 2026.

* [07/07/2025] Our work [URANIA: Differentially Private Insights into AI Use](https://arxiv.org/abs/2506.04681) has been accepted to COLM 2025!

* [06/14/2025] Check out our new work [Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries](https://arxiv.org/abs/2506.07555) on arXiv.

* [05/19/2025] We’ve just uploaded a new version of our paper, [Scaling Embedding Layers in Language Models](https://arxiv.org/abs/2502.01637), on arXiv.


Experience
======

* Google DeepMind - Senior Research Scientist (*Current*)
* Google Research - (Senior) Research Scientist (*August 2024 – December 2025*)
* Microsoft Research Asia - Joint Ph.D. Student / Research Intern (*2019 – 2024*)
    * Mentor: [Prof. Huishuai Zhang](https://huishuai-git.github.io/)
    * Joint Ph.D. Program between MSRA and Sun Yat-sen University.
    * Recipient of the [Microsoft Research PhD Fellowship](https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/articles/eleven-phd-students-are-awarded-the-2021-microsoft-research-asia-fellowship-award/) (2021).

* Sun Yat-sen University - Ph.D. in Computer Science (*2019 – 2024*)
    * Advisors: Prof. [Tie-Yan Liu](https://scholar.google.com/citations?user=Nh832fgAAAAJ&hl=en) and Prof. [Jian Yin](https://openreview.net/profile?id=~Jian_Yin3)

Recent Publications
======

**Please see [Google Scholar](https://scholar.google.com/citations?user=FcRGdiwAAAAJ&hl=en) for an up-to-date list.**

[Gemini 2.5: Pushing the Frontier With Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities](https://arxiv.org/abs/2507.06261)<br>
Gemini team<br>
Technical Report

[Scaling Embedding Layers in Language Models](https://arxiv.org/abs/2502.01637), [[community implementation]](https://github.com/llmsresearch/scone)<br>
**Da Yu**, Edith Cohen, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Daogao Liu, Chiyuan Zhang<br>
NeurIPS 2025

[Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries](https://arxiv.org/abs/2506.07555)<br>
Haoxiang Wang, Zinan Lin, **Da Yu**, Huishuai Zhang<br>
NeurIPS 2025

[URANIA: Differentially Private Insights into AI Use](https://arxiv.org/abs/2506.04681)<br>
Daogao Liu, Edith Cohen, Badih Ghazi, Peter Kairouz, Pritish Kamath, Alexander Knop, Ravi Kumar, Pasin Manurangsi, Adam Sealfon, **Da Yu**, Chiyuan Zhang<br>
COLM 2025

[Selective Pre-training for Private Fine-tuning](https://arxiv.org/abs/2305.13865), [[code]](https://github.com/dayu11/selective_pretraining_for_private_finetuning)<br>
**Da Yu**, Sivakanth Gopi, Janardhan  Kulkarni, Zinan Lin, Saurabh Naik, Tomasz Lukasz Religa, Jian Yin, Huishuai Zhang<br>
TMLR, 2024

[Privacy-Preserving Instructions for Aligning Large Language Models](https://arxiv.org/abs/2402.13659), [[code]](https://github.com/google-research/google-research/tree/master/dp_instructions), [[exp configs]](https://drive.google.com/drive/folders/1ZM5xZoY7thFAhsp9qUDpyf-lJncnd7c5?usp=sharing), [[poster]](https://drive.google.com/file/d/1DMbadZWxfTWDcqkO7n6vultaRPUA5tEC/view?usp=sharing)<br>
**Da Yu**, Peter Kairouz, Sewoong Oh, Zheng Xu<br>
ICML, 2024

[Differentially Private Synthetic Data via Foundation Model APIs 2: Text](https://arxiv.org/abs/2403.01749), [[code]](https://github.com/AI-secure/aug-pe)<br>
Chulin Xie, Zinan Lin, Arturs Backurs, Sivakanth Gopi, **Da Yu**, Huseyin A Inan, Harsha Nori, Haotian Jiang, Huishuai Zhang, Yin Tat Lee, Bo Li, and Sergey Yekhanin<br>
ICML, 2024 (**Spotlight**)

[Individual Privacy Accounting for Differentially Private Stochastic Gradient Descent](https://arxiv.org/abs/2206.02617), [[code]](https://github.com/dayu11/individual_privacy_of_DPSGD)<br>
**Da Yu**, Gautam Kamath, Janardhan Kulkarni, Tie-Yan Liu, Jian Yin, Huishuai Zhang<br>
TMLR, 2023
    
[Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping](https://openreview.net/pdf?id=oze0clVGPeX)<br>
Jiyan He\*, Xuechen Li\*, **Da Yu**\*, Huishuai Zhang, Janardhan Kulkarni, Yin Tat Lee, Arturs Backurs, Nenghai Yu, Jiang Bian<br>
ICLR, 2023

[Differentially Private Fine-tuning of Language Models](https://arxiv.org/abs/2110.06500), [[code]](https://github.com/huseyinatahaninan/Differentially-Private-Fine-tuning-of-Language-Models)<br>
**Da Yu**, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, Huishuai Zhang<br>
ICLR, 2022

[Large Scale Private Learning via Low-rank Reparametrization](https://arxiv.org/abs/2106.09352), [[code]](https://github.com/dayu11/Differentially-Private-Deep-Learning)<br>
**Da Yu**, Huishuai Zhang, Wei Chen, Jian Yin, Tie-Yan Liu<br>
ICML, 2021

[Do not Let Privacy Overbill Utility: Gradient Embedding Perturbation for Private Learning](https://arxiv.org/abs/2102.12677), [[code]](https://github.com/dayu11/Differentially-Private-Deep-Learning/tree/main/vision/GEP)<br>
**Da Yu**\*, Huishuai Zhang\*, Wei Chen, Tie-Yan Liu<br>
ICLR, 2021

[Availability Attacks Create Shortcuts](https://arxiv.org/abs/2111.00898), [[code]](https://github.com/dayu11/Availability-Attacks-Create-Shortcuts)<br>
**Da Yu**, Huishuai Zhang, Wei Chen, Jian Yin, Tie-Yan Liu<br>
KDD, Research Track, 2022

[How Does Data Augmentation Affect Privacy in Machine Learning?](https://arxiv.org/abs/2007.10567), [[code]](https://github.com/dayu11/MI_with_DA)<br>
**Da Yu**, Huishuai Zhang, Wei Chen, Jian Yin, Tie-Yan Liu<br>
AAAI, 2021


Academic Service
======
I am an Area Chaire for ICLR 2026. I am a reviewer for ICML 2022-2025, NeurIPS 2022-2025, and ICLR 2023-2025. I'm awarded as a top reviewer for several times.

